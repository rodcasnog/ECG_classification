{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_METAL_AMP_ENABLED'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "import time\n",
    "from biosppy.signals.ecg import correct_rpeaks\n",
    "from ecgdetectors import Detectors\n",
    "import biosppy.signals.ecg as ecg\n",
    "import cv2\n",
    "import neurokit2 as nk\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# from sklearn.preprocessing import RobustScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_directory = 'extracted_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv(extraction_directory + 'y_train.csv', index_col='id')\n",
    "y_train_np = y_train['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(extraction_directory + 'X_train.csv', index_col='id')\n",
    "X_train_np = X_train.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPOCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "\n",
    "prng = RandomState(0)\n",
    "\n",
    "def augment_signal(signal, max_shift, scaling_factor_range, noise_level, max_freq_change):\n",
    "    signal = signal.copy()\n",
    "    # Time Shifting\n",
    "    if max_shift > 0:\n",
    "        shift = prng.randint(-max_shift, max_shift)\n",
    "        signal = np.roll(signal, shift)\n",
    "\n",
    "    # Scaling\n",
    "    if scaling_factor_range > 0:\n",
    "        signal *= prng.uniform(1 - scaling_factor_range, 1 + scaling_factor_range)\n",
    "\n",
    "    # Adding Noise\n",
    "    if noise_level > 0:\n",
    "        signal += pd.DataFrame(prng.normal(0, noise_level, size=signal.size + 500)).rolling(window=500).median().to_numpy().squeeze()[500:]\n",
    "\n",
    "    # Frequency Change (Resampling)\n",
    "    if max_freq_change > 0:\n",
    "        freq_change = prng.uniform(1 - max_freq_change, 1 + max_freq_change)\n",
    "        resample_size = int(len(signal) * freq_change)\n",
    "        if freq_change > 1:\n",
    "            signal = np.interp(np.linspace(0, 1, resample_size), np.linspace(0, 1, len(signal)), signal)[:signal.size]\n",
    "        elif freq_change < 1:\n",
    "            signal = np.tile(np.interp(np.linspace(0, 1, resample_size), np.linspace(0, 1, len(signal)), signal), 2)[:signal.size]\n",
    "    return signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_signal_ids = dict()\n",
    "for class_label in range(4):\n",
    "    initial_signal_ids[class_label] = []\n",
    "for class_label in range(4):\n",
    "    for i in range(X_train.shape[0]):\n",
    "        if y_train_np[i] == class_label:\n",
    "            initial_signal_ids[class_label].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "0: 3030 = 59.21%\n",
      "1: 443 = 8.66%\n",
      "2: 1474 = 28.81%\n",
      "3: 170 = 3.32%\n",
      "\n",
      "Extended:\n",
      "0: 3030 = 32.65%\n",
      "1: 1772 = 19.09%\n",
      "2: 2948 = 31.77%\n",
      "3: 1530 = 16.49%\n"
     ]
    }
   ],
   "source": [
    "multiplier = 1\n",
    "\n",
    "class_distribution = y_train.apply(lambda x: {i: x[x == i].count() for i in range(4)})['y']\n",
    "print('Original:')\n",
    "for name in class_distribution:\n",
    "    print(str(name) + ': {:} = {:.2%}'.format(class_distribution[name] , class_distribution[name] / y_train_np.size))\n",
    "print('')\n",
    "\n",
    "# Parameters for augmentation\n",
    "max_shift = 400  # Maximum shift for time shifting\n",
    "scaling_factor_range = 0#0.05  # Scaling factor range for scaling\n",
    "noise_level = 500  # Noise level for adding noise\n",
    "max_freq_change = 0.1  # Maximum frequency change for resampling\n",
    "\n",
    "# Augmenting the dataset\n",
    "X_augmented = []\n",
    "y_augmented = []\n",
    "\n",
    "for class_label, repetitions in enumerate([((class_distribution[0] * multiplier ) // (2*v)) for v in class_distribution.values()]):\n",
    "    for _ in range(repetitions):\n",
    "        for index in initial_signal_ids[class_label]:\n",
    "            augmented_signal = augment_signal(X_train_np[i], max_shift, scaling_factor_range, noise_level, max_freq_change)\n",
    "            X_augmented.append(augmented_signal)\n",
    "            y_augmented.append(class_label)\n",
    "\n",
    "# Convert augmented data to numpy arrays and add to original dataset\n",
    "X_train_extended_np = np.array(X_augmented)\n",
    "y_train_extended_np = np.array(y_augmented)\n",
    "\n",
    "X_train_extended_np = np.concatenate((X_train_np, X_augmented))\n",
    "y_train_extended_np = np.concatenate((y_train_np, y_augmented))\n",
    "\n",
    "X_train_extended = pd.DataFrame(X_train_extended_np, columns=X_train.columns)\n",
    "y_train_extended = pd.DataFrame(y_train_extended_np, columns=y_train.columns)\n",
    "\n",
    "class_distribution_extended = y_train_extended.apply(lambda x: {i: x[x == i].count() for i in range(4)})['y']\n",
    "print('Extended:')\n",
    "for name in class_distribution_extended:\n",
    "    print(str(name) + ': {:} = {:.2%}'.format(class_distribution_extended[name], class_distribution_extended[name] / y_train_extended_np.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_signal(signal):\n",
    "\n",
    "    length = signal.size\n",
    "\n",
    "    signal = signal[~np.isnan(signal)]\n",
    "\n",
    "    features = ecg.ecg(signal, 300, show=False)\n",
    "\n",
    "    peaks = correct_rpeaks(features[1], features[2], 300, .15)\n",
    "    median_peak_val = np.median(features[1][peaks])\n",
    "\n",
    "    if median_peak_val == 0 or median_peak_val == np.nan:\n",
    "        raise Exception()\n",
    "    \n",
    "    a = np.empty((length-signal.size,))\n",
    "    a[:] = np.nan\n",
    "    \n",
    "    return np.concatenate([(np.array(features[1]) - np.median(features[1])) / median_peak_val, a])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed_np = np.array([process_signal(row) for row in X_train_extended_np])\n",
    "X_train_transformed = pd.DataFrame(data=X_train_transformed_np, columns=X_train_extended.columns, index=X_train_extended.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGE GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed_with_index = X_train_transformed.copy()\n",
    "X_train_transformed_with_index['index'] = X_train_transformed.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train_cat_np = to_categorical(y_train_extended_np)\n",
    "\n",
    "# Split data\n",
    "X_train_partial, X_val, y_train_partial, y_val = train_test_split(X_train_transformed_with_index.to_numpy(), y_train_cat_np, test_size=0.4, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/Users/rcasado/Desktop/Rodrigo_work/Universidad/PhD/Other/Coding/Visual Studio Code/AML/task2/images'\n",
    "\n",
    "def reset(directory = directory):\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory)\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    for folder in ['train', 'val']:\n",
    "        subdirectory = os.path.join(directory, folder)\n",
    "        os.makedirs(subdirectory)\n",
    "        for i in range(4):\n",
    "            subsubdirectory = os.path.join(subdirectory, str(i))\n",
    "            os.makedirs(subsubdirectory)\n",
    "    \n",
    "    subdirectory = os.path.join(directory, 'test/unkown')\n",
    "    os.makedirs(subdirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use('Agg')\n",
    "\n",
    "def save_heartbeat_images(index, signal, directory, test=False):\n",
    "    \n",
    "    peaks = correct_rpeaks(signal, nk.ecg_peaks(signal, sampling_rate=300)[1]['ECG_R_Peaks'], 300, .15)[0]\n",
    "    \n",
    "    beats = ecg.extract_heartbeats(signal, peaks, 300)['templates']\n",
    "\n",
    "    for i, beat in enumerate(beats):\n",
    "\n",
    "        fig = plt.figure(frameon=False)\n",
    "        plt.plot(beat, linewidth=5)\n",
    "        plt.xticks([]), plt.yticks([])\n",
    "        for spine in plt.gca().spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "        if test:\n",
    "            filename = directory + f'/ECG{index}_heartbeat{i}.png'\n",
    "        else:\n",
    "            filename = directory + '/' + str(y_train_extended_np[index]) + f'/ECG{index}_heartbeat{i}.png'\n",
    "        fig.savefig(filename)\n",
    "        im_gray = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "        im_gray = cv2.resize(im_gray, IMAGE_SIZE, interpolation = cv2.INTER_LANCZOS4)\n",
    "        cv2.imwrite(filename, im_gray)\n",
    "        # cropping(im_gray, filename)\n",
    "        plt.close('all')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, directory=directory + '/train', test=False):\n",
    "    start = time.time()\n",
    "    counter = 0\n",
    "\n",
    "    for row in data:\n",
    "\n",
    "        save_heartbeat_images(int(row[-1]), row[:-1], directory, test)\n",
    "\n",
    "        counter += 1\n",
    "        if counter % 10 == 0:\n",
    "            print('.', end='')\n",
    "        \n",
    "        if counter % 1000 == 0:\n",
    "            print('')\n",
    "\n",
    "    print('\\n' + str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "........................................................\n",
      "6004.830610990524\n"
     ]
    }
   ],
   "source": [
    "reset()\n",
    "\n",
    "process_data(X_train_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      ".......................................................................\n",
      "4008.269727230072\n"
     ]
    }
   ],
   "source": [
    "process_data(X_val, directory + '/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBTAIN TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test  = pd.read_csv(extraction_directory + 'X_test.csv', index_col='id')\n",
    "X_test_np = X_test.to_numpy()\n",
    "\n",
    "X_test_transformed_np = np.array([process_signal(row) for row in X_test_np])\n",
    "X_test_transformed = pd.DataFrame(data=X_test_transformed_np, columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed_with_index = X_test_transformed.copy()\n",
    "X_test_transformed_with_index['index'] = X_test_transformed.index\n",
    "X_test_transformed_with_index_np = X_test_transformed_with_index.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      ".........................................\n",
      "3311.9487721920013\n"
     ]
    }
   ],
   "source": [
    "process_data(X_test_transformed_with_index_np, directory + '/test/unkown', test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3411, 17808)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_transformed_with_index_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "IMAGE_SIZE = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 46s 1us/step\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "inputs = keras.layers.Input(shape=IMAGE_SIZE + (3,))\n",
    "\n",
    "base_model = keras.applications.VGG16(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
    "\n",
    "# base_model = keras.applications.EfficientNetB0(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
    "\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "x = keras.layers.GlobalAveragePooling2D()(base_model.output)   \n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(512, activation='relu')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "outputs = keras.layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "____________________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   Trainable  \n",
      "============================================================================\n",
      " input_6 (InputLayer)        [(None, 128, 128, 3)]     0         N          \n",
      "                                                                            \n",
      " block1_conv1 (Conv2D)       (None, 128, 128, 64)      1792      N          \n",
      "                                                                            \n",
      " block1_conv2 (Conv2D)       (None, 128, 128, 64)      36928     N          \n",
      "                                                                            \n",
      " block1_pool (MaxPooling2D)  (None, 64, 64, 64)        0         N          \n",
      "                                                                            \n",
      " block2_conv1 (Conv2D)       (None, 64, 64, 128)       73856     N          \n",
      "                                                                            \n",
      " block2_conv2 (Conv2D)       (None, 64, 64, 128)       147584    N          \n",
      "                                                                            \n",
      " block2_pool (MaxPooling2D)  (None, 32, 32, 128)       0         N          \n",
      "                                                                            \n",
      " block3_conv1 (Conv2D)       (None, 32, 32, 256)       295168    N          \n",
      "                                                                            \n",
      " block3_conv2 (Conv2D)       (None, 32, 32, 256)       590080    N          \n",
      "                                                                            \n",
      " block3_conv3 (Conv2D)       (None, 32, 32, 256)       590080    N          \n",
      "                                                                            \n",
      " block3_pool (MaxPooling2D)  (None, 16, 16, 256)       0         N          \n",
      "                                                                            \n",
      " block4_conv1 (Conv2D)       (None, 16, 16, 512)       1180160   N          \n",
      "                                                                            \n",
      " block4_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   N          \n",
      "                                                                            \n",
      " block4_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   N          \n",
      "                                                                            \n",
      " block4_pool (MaxPooling2D)  (None, 8, 8, 512)         0         N          \n",
      "                                                                            \n",
      " block5_conv1 (Conv2D)       (None, 8, 8, 512)         2359808   N          \n",
      "                                                                            \n",
      " block5_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   N          \n",
      "                                                                            \n",
      " block5_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   N          \n",
      "                                                                            \n",
      " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         N          \n",
      "                                                                            \n",
      " global_average_pooling2d_5  (None, 512)               0         Y          \n",
      "  (GlobalAveragePooling2D)                                                  \n",
      "                                                                            \n",
      " batch_normalization_6 (Bat  (None, 512)               2048      Y          \n",
      " chNormalization)                                                           \n",
      "                                                                            \n",
      " dropout_8 (Dropout)         (None, 512)               0         Y          \n",
      "                                                                            \n",
      " dense_10 (Dense)            (None, 512)               262656    Y          \n",
      "                                                                            \n",
      " batch_normalization_7 (Bat  (None, 512)               2048      Y          \n",
      " chNormalization)                                                           \n",
      "                                                                            \n",
      " dense_11 (Dense)            (None, 4)                 2052      Y          \n",
      "                                                                            \n",
      "============================================================================\n",
      "Total params: 14983492 (57.16 MB)\n",
      "Trainable params: 266756 (1.02 MB)\n",
      "Non-trainable params: 14716736 (56.14 MB)\n",
      "____________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer=tf.keras.optimizers.legacy.RMSprop(learning_rate=learning_rate))\n",
    "\n",
    "model.summary(show_trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGE GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 230973 images belonging to 4 classes.\n",
      "Found 154705 images belonging to 4 classes.\n",
      "Found 127735 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Set up data generator for training with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.25,\n",
    "    height_shift_range=0.25,\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "# Load images from directories for training\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'images/train',  # Base directory for training images\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    'images/val',  # Base directory for validation images\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'images/test', # Base directory for test images\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=8,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# Set up the ImageDataGenerator with your desired augmentations\n",
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.25,\n",
    "    height_shift_range=0.25,\n",
    ")\n",
    "\n",
    "# Choose a directory with your images\n",
    "img_dir = 'images/test/unkown'  # Change this to your directory\n",
    "\n",
    "# Load one image for demonstration\n",
    "img_path = os.path.join(img_dir, os.listdir(img_dir)[0])\n",
    "img = image.load_img(img_path, target_size=(128, 128), color_mode='grayscale')\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# Generate batches of augmented images from this image\n",
    "aug_iter = datagen.flow(x)\n",
    "\n",
    "# Plot the original and augmented images\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "# Display some augmented images\n",
    "for i in range(3):\n",
    "    plt.subplot(2, 2, i + 2)\n",
    "    aug_img = next(aug_iter)[0].astype('uint8')\n",
    "    plt.imshow(aug_img[:, :, 0], cmap='gray')\n",
    "    plt.title(f'Augmented Image {i+1}')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "7217/7217 [==============================] - 526s 73ms/step - loss: 1.1642 - accuracy: 0.4249 - val_loss: 1.1299 - val_accuracy: 0.4320\n",
      "Epoch 2/25\n",
      "7217/7217 [==============================] - 528s 73ms/step - loss: 1.1297 - accuracy: 0.4318 - val_loss: 1.1152 - val_accuracy: 0.4308\n",
      "Epoch 3/25\n",
      "7217/7217 [==============================] - 526s 73ms/step - loss: 1.1261 - accuracy: 0.4333 - val_loss: 1.1155 - val_accuracy: 0.4334\n",
      "Epoch 4/25\n",
      "7217/7217 [==============================] - 1716s 238ms/step - loss: 1.1233 - accuracy: 0.4356 - val_loss: 1.1292 - val_accuracy: 0.4301\n",
      "Epoch 5/25\n",
      "7217/7217 [==============================] - 525s 73ms/step - loss: 1.1228 - accuracy: 0.4351 - val_loss: 1.1129 - val_accuracy: 0.4363\n",
      "Epoch 6/25\n",
      "7217/7217 [==============================] - 520s 72ms/step - loss: 1.1218 - accuracy: 0.4349 - val_loss: 1.1164 - val_accuracy: 0.4354\n",
      "Epoch 7/25\n",
      "7217/7217 [==============================] - 522s 72ms/step - loss: 1.1216 - accuracy: 0.4355 - val_loss: 1.1188 - val_accuracy: 0.4356\n",
      "Epoch 8/25\n",
      "7217/7217 [==============================] - 522s 72ms/step - loss: 1.1205 - accuracy: 0.4361 - val_loss: 1.1154 - val_accuracy: 0.4326\n",
      "Epoch 9/25\n",
      "7217/7217 [==============================] - 523s 73ms/step - loss: 1.1198 - accuracy: 0.4356 - val_loss: 1.1150 - val_accuracy: 0.4259\n",
      "Epoch 10/25\n",
      "7217/7217 [==============================] - 523s 72ms/step - loss: 1.1197 - accuracy: 0.4365 - val_loss: 1.1180 - val_accuracy: 0.4319\n",
      "Epoch 11/25\n",
      "7217/7217 [==============================] - 521s 72ms/step - loss: 1.1183 - accuracy: 0.4375 - val_loss: 1.1179 - val_accuracy: 0.4302\n",
      "Epoch 12/25\n",
      "7217/7217 [==============================] - 521s 72ms/step - loss: 1.1187 - accuracy: 0.4369 - val_loss: 1.1108 - val_accuracy: 0.4357\n",
      "Epoch 13/25\n",
      "7217/7217 [==============================] - 520s 72ms/step - loss: 1.1189 - accuracy: 0.4359 - val_loss: 1.1172 - val_accuracy: 0.4308\n",
      "Epoch 14/25\n",
      "7217/7217 [==============================] - 520s 72ms/step - loss: 1.1183 - accuracy: 0.4387 - val_loss: 1.1230 - val_accuracy: 0.4298\n",
      "Epoch 15/25\n",
      "7217/7217 [==============================] - 520s 72ms/step - loss: 1.1183 - accuracy: 0.4370 - val_loss: 1.1168 - val_accuracy: 0.4315\n",
      "Epoch 16/25\n",
      "7217/7217 [==============================] - 522s 72ms/step - loss: 1.1191 - accuracy: 0.4369 - val_loss: 1.1101 - val_accuracy: 0.4320\n",
      "Epoch 17/25\n",
      "7217/7217 [==============================] - 521s 72ms/step - loss: 1.1182 - accuracy: 0.4365 - val_loss: 1.1114 - val_accuracy: 0.4346\n",
      "Epoch 18/25\n",
      "7217/7217 [==============================] - 519s 72ms/step - loss: 1.1183 - accuracy: 0.4360 - val_loss: 1.1199 - val_accuracy: 0.4233\n",
      "Epoch 19/25\n",
      "7217/7217 [==============================] - 520s 72ms/step - loss: 1.1187 - accuracy: 0.4366 - val_loss: 1.1101 - val_accuracy: 0.4351\n",
      "Epoch 20/25\n",
      "7217/7217 [==============================] - 521s 72ms/step - loss: 1.1188 - accuracy: 0.4352 - val_loss: 1.1186 - val_accuracy: 0.4202\n",
      "Epoch 21/25\n",
      "7217/7217 [==============================] - 520s 72ms/step - loss: 1.1185 - accuracy: 0.4365 - val_loss: 1.1194 - val_accuracy: 0.4374\n",
      "Epoch 22/25\n",
      "7217/7217 [==============================] - 521s 72ms/step - loss: 1.1188 - accuracy: 0.4369 - val_loss: 1.1150 - val_accuracy: 0.4397\n",
      "Epoch 23/25\n",
      "7217/7217 [==============================] - 522s 72ms/step - loss: 1.1181 - accuracy: 0.4374 - val_loss: 1.1135 - val_accuracy: 0.4307\n",
      "Epoch 24/25\n",
      "7217/7217 [==============================] - 520s 72ms/step - loss: 1.1179 - accuracy: 0.4375 - val_loss: 1.1125 - val_accuracy: 0.4330\n",
      "Epoch 25/25\n",
      "7217/7217 [==============================] - 520s 72ms/step - loss: 1.1175 - accuracy: 0.4362 - val_loss: 1.1120 - val_accuracy: 0.4311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x6e4aaa080>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7217/7217 [==============================] - 531s 73ms/step - loss: 1.1490 - accuracy: 0.4254 - val_loss: 1.1302 - val_accuracy: 0.4262\n",
      "Epoch 2/5\n",
      "7217/7217 [==============================] - 529s 73ms/step - loss: 1.1410 - accuracy: 0.4290 - val_loss: 1.1208 - val_accuracy: 0.4324\n",
      "Epoch 3/5\n",
      "7217/7217 [==============================] - 529s 73ms/step - loss: 1.1411 - accuracy: 0.4299 - val_loss: 1.1681 - val_accuracy: 0.4063\n",
      "Epoch 4/5\n",
      "7217/7217 [==============================] - 532s 74ms/step - loss: 1.1429 - accuracy: 0.4278 - val_loss: 1.1600 - val_accuracy: 0.4040\n",
      "Epoch 5/5\n",
      "7217/7217 [==============================] - 2053s 285ms/step - loss: 1.1437 - accuracy: 0.4270 - val_loss: 1.1289 - val_accuracy: 0.4348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x710971db0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "____________________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   Trainable  \n",
      "============================================================================\n",
      " input_6 (InputLayer)        [(None, 128, 128, 3)]     0         Y          \n",
      "                                                                            \n",
      " block1_conv1 (Conv2D)       (None, 128, 128, 64)      1792      Y          \n",
      "                                                                            \n",
      " block1_conv2 (Conv2D)       (None, 128, 128, 64)      36928     Y          \n",
      "                                                                            \n",
      " block1_pool (MaxPooling2D)  (None, 64, 64, 64)        0         Y          \n",
      "                                                                            \n",
      " block2_conv1 (Conv2D)       (None, 64, 64, 128)       73856     Y          \n",
      "                                                                            \n",
      " block2_conv2 (Conv2D)       (None, 64, 64, 128)       147584    Y          \n",
      "                                                                            \n",
      " block2_pool (MaxPooling2D)  (None, 32, 32, 128)       0         Y          \n",
      "                                                                            \n",
      " block3_conv1 (Conv2D)       (None, 32, 32, 256)       295168    Y          \n",
      "                                                                            \n",
      " block3_conv2 (Conv2D)       (None, 32, 32, 256)       590080    Y          \n",
      "                                                                            \n",
      " block3_conv3 (Conv2D)       (None, 32, 32, 256)       590080    Y          \n",
      "                                                                            \n",
      " block3_pool (MaxPooling2D)  (None, 16, 16, 256)       0         Y          \n",
      "                                                                            \n",
      " block4_conv1 (Conv2D)       (None, 16, 16, 512)       1180160   Y          \n",
      "                                                                            \n",
      " block4_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   Y          \n",
      "                                                                            \n",
      " block4_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   Y          \n",
      "                                                                            \n",
      " block4_pool (MaxPooling2D)  (None, 8, 8, 512)         0         Y          \n",
      "                                                                            \n",
      " block5_conv1 (Conv2D)       (None, 8, 8, 512)         2359808   Y          \n",
      "                                                                            \n",
      " block5_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   Y          \n",
      "                                                                            \n",
      " block5_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   Y          \n",
      "                                                                            \n",
      " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         Y          \n",
      "                                                                            \n",
      " global_average_pooling2d_5  (None, 512)               0         Y          \n",
      "  (GlobalAveragePooling2D)                                                  \n",
      "                                                                            \n",
      " batch_normalization_6 (Bat  (None, 512)               2048      Y          \n",
      " chNormalization)                                                           \n",
      "                                                                            \n",
      " dropout_8 (Dropout)         (None, 512)               0         Y          \n",
      "                                                                            \n",
      " dense_10 (Dense)            (None, 512)               262656    Y          \n",
      "                                                                            \n",
      " batch_normalization_7 (Bat  (None, 512)               2048      Y          \n",
      " chNormalization)                                                           \n",
      "                                                                            \n",
      " dense_11 (Dense)            (None, 4)                 2052      Y          \n",
      "                                                                            \n",
      "============================================================================\n",
      "Total params: 14983492 (57.16 MB)\n",
      "Trainable params: 14981444 (57.15 MB)\n",
      "Non-trainable params: 2048 (8.00 KB)\n",
      "____________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def unfreeze_model(model):\n",
    "    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n",
    "    for layer in model.layers:\n",
    "        if not isinstance(layer, keras.layers.BatchNormalization):\n",
    "            layer.trainable = True\n",
    "\n",
    "\n",
    "unfreeze_model(model)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-5))\n",
    "\n",
    "model.summary(show_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7217/7217 [==============================] - 915s 127ms/step - loss: 1.0434 - accuracy: 0.4650 - val_loss: 1.0248 - val_accuracy: 0.4577 - lr: 1.0000e-05\n",
      "Epoch 2/10\n",
      "7217/7217 [==============================] - 955s 132ms/step - loss: 1.0038 - accuracy: 0.4806 - val_loss: 1.0188 - val_accuracy: 0.4600 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "7217/7217 [==============================] - 967s 134ms/step - loss: 0.9838 - accuracy: 0.4908 - val_loss: 1.0166 - val_accuracy: 0.4737 - lr: 1.0000e-05\n",
      "Epoch 4/10\n",
      "7217/7217 [==============================] - 991s 137ms/step - loss: 0.9684 - accuracy: 0.4996 - val_loss: 1.0069 - val_accuracy: 0.4813 - lr: 1.0000e-05\n",
      "Epoch 5/10\n",
      "7217/7217 [==============================] - 980s 136ms/step - loss: 0.9547 - accuracy: 0.5069 - val_loss: 1.0177 - val_accuracy: 0.4705 - lr: 1.0000e-05\n",
      "Epoch 6/10\n",
      "7217/7217 [==============================] - 993s 138ms/step - loss: 0.9420 - accuracy: 0.5133 - val_loss: 1.0212 - val_accuracy: 0.4764 - lr: 1.0000e-05\n",
      "Epoch 7/10\n",
      "7217/7217 [==============================] - 1017s 141ms/step - loss: 0.9314 - accuracy: 0.5186 - val_loss: 1.0134 - val_accuracy: 0.4762 - lr: 1.0000e-05\n",
      "Epoch 8/10\n",
      "7217/7217 [==============================] - 993s 138ms/step - loss: 0.9184 - accuracy: 0.5261 - val_loss: 1.0275 - val_accuracy: 0.4751 - lr: 1.0000e-05\n",
      "Epoch 9/10\n",
      "7217/7217 [==============================] - 1006s 139ms/step - loss: 0.9081 - accuracy: 0.5304 - val_loss: 1.0364 - val_accuracy: 0.4727 - lr: 1.0000e-05\n",
      "Epoch 10/10\n",
      "7217/7217 [==============================] - 1012s 140ms/step - loss: 0.8978 - accuracy: 0.5340 - val_loss: 1.0279 - val_accuracy: 0.4712 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x78d6be440>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a learning rate decay method:\n",
    "lr_decay = keras.callbacks.ReduceLROnPlateau(monitor='loss', \n",
    "                             patience=1,\n",
    "                             verbose=0,\n",
    "                             factor=0.5,\n",
    "                             min_lr=1e-8)\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=10,\n",
    "    callbacks=[lr_decay]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "test_images_path = directory + '/test/unkown'\n",
    "predictions = {}\n",
    "\n",
    "for idx in range(X_test_transformed_with_index_np.shape[0]):\n",
    "\n",
    "    heartbeat_predictions = []\n",
    "    heartbeats = []\n",
    "\n",
    "    for heartbeat_idx in range(120):\n",
    "        img_name = f'ECG{idx}_heartbeat{heartbeat_idx}.png'\n",
    "        img_path = os.path.join(test_images_path, img_name)\n",
    "\n",
    "        if os.path.exists(img_path):\n",
    "            heartbeats.append(np.repeat(cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)[..., np.newaxis], 3, axis=-1) / 255)\n",
    "    \n",
    "    heartbeat_predictions.append(np.argmax(model( np.array(heartbeats), training=False), axis=1)[0])\n",
    "\n",
    "    median_prediction = int(stats.mode(np.array(heartbeat_predictions)).mode)\n",
    "    predictions[idx] = median_prediction\n",
    "\n",
    "y_pred = pd.DataFrame(data={'y': predictions})\n",
    "y_pred.index.name = 'id'\n",
    "\n",
    "y_pred.to_csv('sol_2D.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': 'y'}>]], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
